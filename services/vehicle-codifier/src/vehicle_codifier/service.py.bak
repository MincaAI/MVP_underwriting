"""Core vehicle codification service with pgvector similarity search."""

import time
from typing import List, Optional, Tuple
from sqlalchemy import create_engine, text
from sqlalchemy.orm import Session
import numpy as np
from rapidfuzz import fuzz

from .config import get_settings
from .models import VehicleInput, MatchResult, Candidate, ExtractedFields, ReviewCandidate
from .preprocessor import VehiclePreprocessor
from .extractor import VehicleExtractor
from .label_builder import VehicleLabelBuilder
from .cache import VehicleCatalogCache


class VehicleCodeifier:
    """Simplified vehicle codification service using pgvector similarity."""

    def __init__(self):
        self.settings = get_settings()
        self.engine = create_engine(self.settings.database_url)
        self.preprocessor = VehiclePreprocessor()
        self.extractor = VehicleExtractor()
        self.label_builder = VehicleLabelBuilder()
        self.cache = VehicleCatalogCache()

    def match_vehicle(self, vehicle_input: VehicleInput) -> MatchResult:
        """Match a single vehicle against the CATVER catalog."""
        start_time = time.time()

        try:
            # Step 1: Preprocess input (smart field detection and normalization)
            raw_input = {"modelo": vehicle_input.modelo, "description": vehicle_input.description}
            processed = self.preprocessor.process_single(raw_input)

            # Step 2: Extract CATVER fields from normalized description
            extracted_fields = self.extractor.extract_fields(processed["description"])

            # Step 3: Build query label and generate embedding
            query_label, embedding = self.label_builder.build_label_and_embedding(
                processed["model_year"], extracted_fields
            )

            # Step 4: Find candidates using hybrid approach (cache + database fallback)
            if embedding:
                candidates = self._find_candidates_hybrid(
                    query_label, extracted_fields, processed["model_year"], embedding
                )
            else:
                candidates = self._find_candidates_fallback(extracted_fields, processed["model_year"])

            # Step 4: Apply decision thresholds
            decision, confidence, suggested_cvegs = self._make_decision(candidates, extracted_fields)

            # Step 5: Create enhanced response with review candidates
            top_candidates_for_review = self._create_review_candidates(candidates, decision)
            recommendation = self._get_recommendation(decision, confidence)

            processing_time_ms = (time.time() - start_time) * 1000

            return MatchResult(
                success=decision != "no_match",
                decision=decision,
                confidence=confidence,
                suggested_cvegs=suggested_cvegs,
                candidates=candidates[:self.settings.max_results],
                extracted_fields=extracted_fields,
                processing_time_ms=processing_time_ms,
                query_label=query_label,
                top_candidates_for_review=top_candidates_for_review,
                recommendation=recommendation
            )

        except Exception as e:
            processing_time_ms = (time.time() - start_time) * 1000
            return MatchResult(
                success=False,
                decision="no_match",
                confidence=0.0,
                candidates=[],
                processing_time_ms=processing_time_ms,
                query_label=f"Error: {str(e)}"
            )

    def _find_candidates_hybrid(
        self, query_label: str, fields: ExtractedFields, modelo: int, embedding: List[float]
    ) -> List[Candidate]:
        """Find candidates using hybrid approach: cache first, database fallback."""

        # Check if cache should be refreshed
        if self.cache.should_refresh_cache():
            print("ðŸ”„ Cache refresh needed, refreshing in background...")
            self.cache.refresh_cache()

        # Try cache first
        if self.cache.is_cache_available():
            try:
                print("ðŸš€ Using in-memory cache for matching...")
                # Use exact year or range based on variance setting
                if self.settings.year_variance == 0:
                    year_min = year_max = modelo
                else:
                    year_min = modelo - self.settings.year_variance
                    year_max = modelo + self.settings.year_variance

                return self.cache.find_candidates_cached(
                    query_embedding=embedding,
                    query_label=query_label,
                    year_min=year_min,
                    year_max=year_max
                )
            except Exception as e:
                print(f"âš ï¸ Cache search failed, falling back to database: {e}")
                # Fall through to database search

        # Database fallback
        print("ðŸ—„ï¸ Using database fallback for matching...")
        return self._find_candidates_with_embedding(query_label, fields, modelo, embedding)

    def _find_candidates_with_embedding(
        self, query_label: str, fields: ExtractedFields, modelo: int, embedding: List[float]
    ) -> List[Candidate]:
        """Find candidates using pgvector similarity search."""
        # Build SQL query with vector similarity + year filter
        sql_params = {
            'qvec': str(embedding),
            'year': modelo
        }

        # Build SQL with year filter
        where_conditions = [
            "catalog_version = (SELECT version FROM catalog_import WHERE status IN ('ACTIVE', 'LOADED') ORDER BY version DESC LIMIT 1)",
            "embedding IS NOT NULL"
        ]

        # Use exact year matching or range based on variance setting
        if modelo is not None:
            if self.settings.year_variance == 0:
                where_conditions.append("modelo = :year")
            else:
                sql_params['ymin'] = modelo - self.settings.year_variance
                sql_params['ymax'] = modelo + self.settings.year_variance
                where_conditions.append("modelo BETWEEN :ymin AND :ymax")

        sql = f"""
        SELECT cvegs, marca, submarca, modelo, descveh, label,
               1 - (embedding <=> CAST(:qvec AS vector)) AS similarity_score
        FROM amis_catalog
        WHERE {' AND '.join(where_conditions)}
        ORDER BY embedding <=> CAST(:qvec AS vector)
        LIMIT :limit
        """

        sql_params['limit'] = self.settings.max_candidates

        with Session(self.engine) as session:
            result = session.execute(text(sql), sql_params)
            rows = result.fetchall()

        # Convert to candidates with enhanced multi-factor scoring
        candidates = []
        for row in rows:
            # Calculate all scoring factors
            fuzzy_score = self._calculate_fuzzy_score(query_label, row.label)
            brand_score = self._calculate_brand_match_score(fields, row.marca)
            year_score = self._calculate_year_proximity_score(modelo, row.modelo)
            type_score = self._calculate_type_match_score(fields, row.descveh)

            # Enhanced multi-factor final score
            final_score = (
                self.settings.weight_embedding * row.similarity_score +
                self.settings.weight_fuzzy * fuzzy_score +
                self.settings.weight_brand_match * brand_score +
                self.settings.weight_year_proximity * year_score +
                self.settings.weight_type_match * type_score
            )

            candidates.append(Candidate(
                cvegs=row.cvegs,
                marca=row.marca,
                submarca=row.submarca,
                modelo=row.modelo,
                descveh=row.descveh,
                label=row.label,
                similarity_score=row.similarity_score,
                fuzzy_score=fuzzy_score,
                final_score=final_score
            ))

        # Re-sort by final hybrid score
        candidates.sort(key=lambda x: x.final_score, reverse=True)
        return candidates

    def _find_candidates_fallback(
        self, fields: ExtractedFields, modelo: int
    ) -> List[Candidate]:
        """Fallback candidate search without embeddings."""

        sql_params = {
            'model': fields.submarca if fields.submarca else None,
            'year': modelo if modelo else None
        }

        # Build SQL dynamically based on available filters
        where_conditions = [
            "catalog_version = (SELECT version FROM catalog_import WHERE status IN ('ACTIVE', 'LOADED') ORDER BY version DESC LIMIT 1)"
        ]

        if sql_params['model']:
            where_conditions.append("submarca ILIKE '%' || :model || '%'")
        if sql_params['year']:
            where_conditions.append("modelo = :year")

        sql = f"""
        SELECT cvegs, marca, submarca, modelo, descveh, label
        FROM amis_catalog
        WHERE {' AND '.join(where_conditions)}
        ORDER BY marca, submarca, modelo
        LIMIT :limit
        """

        sql_params['limit'] = self.settings.max_candidates

        with Session(self.engine) as session:
            result = session.execute(text(sql), sql_params)
            rows = result.fetchall()

        # Create candidates with fuzzy scoring only
        candidates = []
        query_text = f"{fields.marca or ''} {fields.submarca or ''} {modelo or ''}".strip()

        for row in rows:
            catalog_text = f"{row.marca} {row.submarca} {row.modelo}"
            fuzzy_score = fuzz.ratio(query_text.lower(), catalog_text.lower()) / 100.0

            candidates.append(Candidate(
                cvegs=row.cvegs,
                marca=row.marca,
                submarca=row.submarca,
                modelo=row.modelo,
                descveh=row.descveh,
                label=row.label,
                similarity_score=0.0,  # No embedding similarity available
                fuzzy_score=fuzzy_score,
                final_score=fuzzy_score
            ))

        candidates.sort(key=lambda x: x.final_score, reverse=True)
        return candidates

    def _calculate_fuzzy_score(self, query_label: str, catalog_label: str) -> float:
        """Calculate vehicle-aware fuzzy string similarity score."""
        query_clean = self._normalize_vehicle_text(query_label.lower())
        catalog_clean = self._normalize_vehicle_text(catalog_label.lower())

        # Use multiple fuzzy algorithms and take the best score
        ratio_score = fuzz.ratio(query_clean, catalog_clean) / 100.0
        partial_score = fuzz.partial_ratio(query_clean, catalog_clean) / 100.0
        token_sort_score = fuzz.token_sort_ratio(query_clean, catalog_clean) / 100.0

        # Weight the scores (ratio for overall, partial for substring, token_sort for word order)
        weighted_score = (
            0.5 * ratio_score +
            0.3 * partial_score +
            0.2 * token_sort_score
        )

        return weighted_score

    def _normalize_vehicle_text(self, text: str) -> str:
        """Normalize vehicle text for better fuzzy matching."""
        import re

        # Remove extra whitespace and special characters
        text = re.sub(r'\s+', ' ', text.strip())
        text = re.sub(r'[^\w\s]', ' ', text)

        # Common vehicle term normalizations
        normalizations = {
            # Brand standardizations
            'volkswagen': 'vw',
            'chevrolet': 'chevy',
            'mercedes-benz': 'mercedes',
            'mercedes benz': 'mercedes',
            'bmw': 'bmw',

            # Model standardizations
            'tracto': 'tractor',
            'camioneta': 'pickup',
            'automovil': 'auto',
            'motocicleta': 'motorcycle',

            # Year normalizations
            'modelo': 'model',
            'aÃ±o': 'year',

            # Remove common noise words
            'tr ': '',
            'veh ': '',
            'unidad ': '',
        }

        for old_term, new_term in normalizations.items():
            text = text.replace(old_term, new_term)

        # Remove extra spaces again after replacements
        text = re.sub(r'\s+', ' ', text.strip())

        return text

    def _calculate_brand_match_score(self, fields: ExtractedFields, catalog_marca: str) -> float:
        """Calculate brand exact match bonus score."""
        if not fields.marca or not catalog_marca:
            return 0.0

        query_brand = fields.marca.lower().strip()
        catalog_brand = catalog_marca.lower().strip()

        # Exact match bonus
        if query_brand == catalog_brand:
            return 1.0

        # Partial match for common abbreviations/variations
        if query_brand in catalog_brand or catalog_brand in query_brand:
            return 0.5

        return 0.0

    def _calculate_year_proximity_score(self, query_year: int, catalog_year: int) -> float:
        """Calculate year proximity bonus score."""
        if not query_year or not catalog_year:
            return 0.0

        year_diff = abs(query_year - catalog_year)

        # Perfect match
        if year_diff == 0:
            return 1.0
        # Close years (within variance)
        elif year_diff <= self.settings.year_variance:
            return 1.0 - (year_diff / self.settings.year_variance) * 0.5
        # Distant years
        else:
            return 0.0

    def _calculate_type_match_score(self, fields: ExtractedFields, catalog_descveh: str) -> float:
        """Calculate vehicle type match bonus score."""
        if not fields.tipveh or not catalog_descveh:
            return 0.0

        query_type = fields.tipveh.lower().strip()
        catalog_desc = catalog_descveh.lower().strip()

        # Vehicle type mapping for better matching
        type_keywords = {
            'auto': ['sedan', 'hatchback', 'coupe', 'convertible'],
            'camioneta': ['pickup', 'truck', 'tracto', 'trailer', 'cargo'],
            'motocicleta': ['motorcycle', 'moto', 'scooter'],
            'suv': ['suv', 'crossover', 'jeep']
        }

        # Direct type match in description
        if query_type in catalog_desc:
            return 1.0

        # Check type keywords
        for veh_type, keywords in type_keywords.items():
            if query_type == veh_type:
                for keyword in keywords:
                    if keyword in catalog_desc:
                        return 0.8

        return 0.0

    def _make_decision(self, candidates: List[Candidate], fields: ExtractedFields) -> Tuple[str, float, Optional[int]]:
        """Apply dynamic decision thresholds based on vehicle type."""
        if not candidates:
            return "no_match", 0.0, None

        best_candidate = candidates[0]
        confidence = best_candidate.final_score

        # Get dynamic thresholds based on vehicle type
        thresholds = self._get_dynamic_thresholds(fields.tipveh)
        threshold_high = thresholds["high"]
        threshold_low = thresholds["low"]

        if confidence >= threshold_high:
            return "auto_accept", confidence, best_candidate.cvegs
        elif confidence >= threshold_low:
            return "needs_review", confidence, best_candidate.cvegs
        else:
            return "no_match", confidence, None

    def _get_dynamic_thresholds(self, vehicle_type: Optional[str]) -> dict:
        """Get dynamic thresholds based on vehicle type."""
        if not vehicle_type:
            return self.settings.thresholds_by_type["default"]

        vehicle_type_clean = vehicle_type.lower().strip()

        # Map vehicle types to threshold categories
        if vehicle_type_clean in ["auto", "sedan", "hatchback", "coupe"]:
            return self.settings.thresholds_by_type["auto"]
        elif vehicle_type_clean in ["camioneta", "pickup", "truck", "tracto", "trailer"]:
            return self.settings.thresholds_by_type["camioneta"]
        elif vehicle_type_clean in ["motocicleta", "motorcycle", "moto"]:
            return self.settings.thresholds_by_type["motocicleta"]
        else:
            return self.settings.thresholds_by_type["default"]

    def _create_review_candidates(self, candidates: List[Candidate], decision: str) -> List[ReviewCandidate]:
        """Create review candidates for enhanced user experience."""
        if not self.settings.return_candidates_on_no_match and decision == "no_match":
            return []

        if not candidates:
            return []

        # Determine how many candidates to return
        num_candidates = min(
            len(candidates),
            self.settings.max_candidates_for_review if decision == "no_match"
            else self.settings.min_candidates_for_review
        )

        review_candidates = []
        for candidate in candidates[:num_candidates]:
            match_quality = self._determine_match_quality(candidate.final_score, decision)

            review_candidate = ReviewCandidate(
                cvegs=candidate.cvegs,
                marca=candidate.marca,
                submarca=candidate.submarca,
                modelo=candidate.modelo,
                descveh=candidate.descveh,
                confidence=candidate.final_score,
                match_quality=match_quality,
                similarity_score=candidate.similarity_score,
                fuzzy_score=candidate.fuzzy_score
            )
            review_candidates.append(review_candidate)

        return review_candidates

    def _determine_match_quality(self, confidence: float, decision: str) -> str:
        """Determine match quality based on confidence score and decision."""
        if decision == "auto_accept":
            return "high"
        elif decision == "needs_review":
            return "medium"
        else:  # no_match
            if confidence >= 0.4:
                return "medium"
            elif confidence >= 0.2:
                return "low"
            else:
                return "very_low"

    def _get_recommendation(self, decision: str, confidence: float) -> str:
        """Get recommendation for user action."""
        if decision == "auto_accept":
            return "use_suggested_cvegs"
        elif decision == "needs_review":
            return "manual_review_recommended"
        else:  # no_match
            if confidence >= 0.3:
                return "manual_review_suggested"
            else:
                return "manual_entry_may_be_needed"

    def get_health_status(self) -> dict:
        """Get service health status."""
        try:
            with Session(self.engine) as session:
                # Check database connection
                session.execute(text("SELECT 1"))

                # Get active catalog info
                result = session.execute(text("""
                    SELECT ci.version, COUNT(ac.id) as record_count
                    FROM catalog_import ci
                    LEFT JOIN amis_catalog ac ON ac.catalog_version = ci.version
                    WHERE ci.status IN ('ACTIVE', 'LOADED')
                    GROUP BY ci.version
                    ORDER BY ci.version DESC
                    LIMIT 1
                """))

                row = result.fetchone()
                active_version = row.version if row else None
                record_count = row.record_count if row else 0

                # Get component health status
                preprocessor_health = self.preprocessor.get_health_status()
                extractor_health = self.extractor.get_health_status()
                label_builder_health = self.label_builder.get_health_status()
                cache_health = self.cache.get_health_status()

                return {
                    "status": "healthy",
                    "database_connected": True,
                    "active_catalog_version": active_version,
                    "catalog_records": record_count,
                    "embedding_model": label_builder_health["embedding_model"],
                    "embedder_available": label_builder_health["embedder_available"],
                    "openai_available": extractor_health["openai_available"],
                    "preprocessor_healthy": True,
                    "extractor_healthy": True,
                    "label_builder_healthy": True,
                    "cache_healthy": cache_health["cache_healthy"],
                    "cache_enabled": cache_health["cache_enabled"],
                    "cache_loaded": cache_health["cache_loaded"],
                    "cache_record_count": cache_health["record_count"],
                    "cache_memory_usage_mb": cache_health["memory_usage_mb"],
                    "cache_last_refresh": cache_health["last_refresh"],
                    "cache_needs_refresh": cache_health["needs_refresh"]
                }

        except Exception as e:
            extractor_health = self.extractor.get_health_status()
            label_builder_health = self.label_builder.get_health_status()
            cache_health = self.cache.get_health_status()
            return {
                "status": "unhealthy",
                "database_connected": False,
                "error": str(e),
                "embedding_model": label_builder_health["embedding_model"],
                "embedder_available": label_builder_health["embedder_available"],
                "openai_available": extractor_health["openai_available"],
                "preprocessor_healthy": False,
                "extractor_healthy": False,
                "label_builder_healthy": False,
                "cache_healthy": cache_health.get("cache_healthy", False),
                "cache_enabled": cache_health.get("cache_enabled", False),
                "cache_loaded": cache_health.get("cache_loaded", False)
            }